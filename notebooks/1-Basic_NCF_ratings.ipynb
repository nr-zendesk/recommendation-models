{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b91e8dc4-2b69-4dbf-bc91-d530ae34cb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "acho\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1259b19f-57ba-4d26-a1bc-0cc9029902b3",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "The most basic Neural Matrix Factorization model possible based on Figure 3 from the original [NCF paper](https://arxiv.org/pdf/1708.05031).\n",
    "\n",
    "This model is trained on the Movielense dataset where the objective is to predict ratings different users give to movies.\n",
    "\n",
    "This ultra-simplistic version uses only the user and movie IDs to create learnable embeddings, no additional user or item features are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0164411-73a5-4718-8e4c-c63462b5749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define the general parameters that will be used.\n",
    "Seeds are set for reproducibility.\n",
    "Device is set to select if we will run the model on a GPU or on CPU.\n",
    "All other parameters are related to the model itseld, it's optimizer and training schema.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-6\n",
    "batch_size = 2048\n",
    "epochs = 10\n",
    "hidden_layers = [32, 32]\n",
    "embedding_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54b562e7-7278-405f-9a99-d40618b91484",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Select which dataset to use.\n",
    "The files in each dataset are stored differently mostly due to data size but also as its structure evolved over the years.\n",
    "\"\"\"\n",
    "\n",
    "dataset = 'ml-20m'\n",
    "\n",
    "if dataset == 'ml-100k':\n",
    "    data = pd.read_csv(f'../data/{dataset}/u.data', sep=\"\\t\", header=None)\n",
    "    data.columns = ['user id', 'movie id', 'rating', 'timestamp']\n",
    "elif dataset == 'ml-20m':\n",
    "    data = pd.read_csv(f'../data/{dataset}/ratings.csv',on_bad_lines='skip')\n",
    "else:\n",
    "    print(f'{dataset} does nto exist or was incorrectly written')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce23f09b-bbdb-42e0-9027-ab2aee65852b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1112486027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1112484676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1112484819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1112484727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1112484580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000258</th>\n",
       "      <td>138493</td>\n",
       "      <td>68954</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1258126920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000259</th>\n",
       "      <td>138493</td>\n",
       "      <td>69526</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1259865108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000260</th>\n",
       "      <td>138493</td>\n",
       "      <td>69644</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1260209457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000261</th>\n",
       "      <td>138493</td>\n",
       "      <td>70286</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1258126944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000262</th>\n",
       "      <td>138493</td>\n",
       "      <td>71619</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1255811136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000263 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          userId  movieId  rating   timestamp\n",
       "0              1        2     3.5  1112486027\n",
       "1              1       29     3.5  1112484676\n",
       "2              1       32     3.5  1112484819\n",
       "3              1       47     3.5  1112484727\n",
       "4              1       50     3.5  1112484580\n",
       "...          ...      ...     ...         ...\n",
       "20000258  138493    68954     4.5  1258126920\n",
       "20000259  138493    69526     4.5  1259865108\n",
       "20000260  138493    69644     3.0  1260209457\n",
       "20000261  138493    70286     5.0  1258126944\n",
       "20000262  138493    71619     2.5  1255811136\n",
       "\n",
       "[20000263 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90a348e4-6f4a-4584-a520-af0dba3dc34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The embedding layers expect indices to start at 0.\n",
    "Given both user and movie IDs start at 1, we apply label encoder to fix it. \n",
    "Subtracting 1 would also solve the problem however that would be problem specific and not a general solution.\n",
    "\"\"\"\n",
    "\n",
    "user_enc = LabelEncoder()\n",
    "item_enc = LabelEncoder()\n",
    "data['user_id_enc'] = user_enc.fit_transform(data['userId'])\n",
    "data['movie_id_enc'] = item_enc.fit_transform(data['movieId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06a9553f-e27f-4500-aead-6b936fd72e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class for loading user-item interaction data for the NCF model.\n",
    "\n",
    "    This class is designed to handle datasets containing user-item pairs and their associated ratings.\n",
    "    It converts the input data into a format suitable for PyTorch's data loading utilities.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    user_item_pairs : np.ndarray\n",
    "        A numpy array containing user-item pairs, where each pair consists of encoded user IDs and movie IDs.\n",
    "    \n",
    "    ratings : np.ndarray\n",
    "        A numpy array containing the normalized ratings (scaled between 0 and 1) corresponding to each user-item pair.\n",
    "    \n",
    "    pair_max_ids : np.ndarray\n",
    "        A numpy array containing the maximum user and movie IDs in the dataset, used for indexing purposes.\n",
    "\n",
    "    Methods:\n",
    "    -------\n",
    "    __getitem__(index):\n",
    "        Retrieves the user ID, movie ID, and corresponding rating for a specified index.\n",
    "    \n",
    "    __len__():\n",
    "        Returns the total number of ratings in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        A pandas DataFrame containing the dataset with at least the following columns 'user_id_enc', 'movie_id_enc', and 'rating'.\n",
    "        - 'user_id_enc': Encoded user IDs.\n",
    "        - 'movie_id_enc': Encoded movie IDs.\n",
    "        - 'rating': Ratings given by users to the movies, expected to be in the range [0, 5].\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> dataset = MLDataset(data)\n",
    "    >>> user_id, movie_id, rating = dataset[0]\n",
    "    >>> dataset_length = len(dataset)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.user_item_pairs = data[['user_id_enc', 'movie_id_enc']].to_numpy().astype(np.int32)\n",
    "        self.ratings = (data[['rating']]/5.).to_numpy().astype(np.float32)\n",
    "        self.pair_max_ids = np.max(self.user_item_pairs, axis=0)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.user_item_pairs[index][0], self.user_item_pairs[index][1], self.ratings[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ratings.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df926d2c-2795-4709-9037-6a80d14524b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create the train, validation and test set partitions.\n",
    "Pytorch's random_split works similarly to scikit train_test_split, but allows for 2 additional important things:\n",
    " - Can partition instanciated pytorch Datasets.\n",
    " - Can split 3 ways, instead of only 2.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "dataset = MLDataset(data)\n",
    "\n",
    "train_length = int(len(dataset) * 0.7)\n",
    "valid_length = int(len(dataset) * 0.2)\n",
    "test_length = len(dataset) - train_length - valid_length\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = random_split(dataset, (train_length, valid_length, test_length))\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=7)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=7)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=1, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "788800fc-b558-4c4e-8e6d-cf43cc204a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Collaborative Filtering (NCF) model.\n",
    "\n",
    "    This class implements a neural network model for collaborative filtering, combining user and item embeddings \n",
    "    with a multi-layer perceptron (MLP) and a Generalized Matrix Factorization (GMF) to predict user-item interactions.\n",
    "    The model uses embeddings to capture the latent factors of users and items, and then applies both a nonlinear transformation\n",
    "    through the MLP, and a linear transformation through the GMF.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    user_embedding : nn.Embedding\n",
    "        An embedding layer for user IDs, mapping each user to a dense vector representation of specified size.\n",
    "    \n",
    "    item_embedding : nn.Embedding\n",
    "        An embedding layer for item IDs, mapping each item to a dense vector representation of specified size.\n",
    "    \n",
    "    mlp : nn.ModuleList\n",
    "        A list of sequential layers forming the multi-layer perceptron, which processes the concatenated embeddings \n",
    "        to learn non-linear interactions between users and items.\n",
    "    \n",
    "    output : nn.Linear\n",
    "        A linear layer that produces the final prediction score for user-item interactions.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    num_users : int\n",
    "        The total number of unique users in the dataset.\n",
    "    \n",
    "    num_items : int\n",
    "        The total number of unique items in the dataset.\n",
    "    \n",
    "    embedding_size : int, optional\n",
    "        The size of the user and item embeddings (default is 32).\n",
    "    \n",
    "    hidden_layers : list of int, optional\n",
    "        A list specifying the number of neurons in each hidden layer of the MLP (default is [32, 32]).\n",
    "\n",
    "    Methods:\n",
    "    -------\n",
    "    forward(user_input, item_input):\n",
    "        Defines the forward pass of the model, computing the prediction for the given user and item inputs.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> model = NCF(num_users=1000, num_items=500, embedding_size=32, hidden_layers=[64, 32])\n",
    "    >>> user_input = torch.tensor([0, 1, 2])\n",
    "    >>> item_input = torch.tensor([5, 6, 7])\n",
    "    >>> predictions = model(user_input, item_input)\n",
    "    >>> print(predictions.shape)  # Output: torch.Size([3, 1])\n",
    "    \"\"\"\n",
    "    def __init__(self, num_users, num_items, embedding_size=32, hidden_layers=[32,32]):\n",
    "        super(NCF, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_size)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_size)\n",
    "\n",
    "        input_dim = 2 * embedding_size\n",
    "        self.mlp = nn.ModuleList()\n",
    "        for hidden_dim in hidden_layers:\n",
    "            self.mlp.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(input_dim, hidden_dim),\n",
    "                    nn.BatchNorm1d(hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(p=0.2)\n",
    "                )\n",
    "            )\n",
    "            input_dim = hidden_dim\n",
    "\n",
    "        self.output = nn.Linear(2 * embedding_size, 1)\n",
    "        \n",
    "    def forward(self, user_input, item_input):\n",
    "        user_embedded = self.user_embedding(user_input)\n",
    "        item_embedded = self.item_embedding(item_input)\n",
    "        gmf = user_embedded * item_embedded\n",
    "        x = torch.cat([user_embedded, item_embedded], dim=-1)\n",
    "        for mlp_layer in self.mlp:\n",
    "            x = mlp_layer(x)\n",
    "        x = torch.cat([gmf, x], dim=-1)\n",
    "        prediction = self.output(x)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8ae282c-1286-4e76-8303-dad82a0007f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, data_loader, criterion, device, log_interval=100):\n",
    "    \"\"\"\n",
    "    Train the given model using the provided data loader and optimization parameters.\n",
    "\n",
    "    This function performs one epoch of training for the specified model, iterating over the data loader to \n",
    "    retrieve user-item interactions and their corresponding ratings. It computes the loss using the specified \n",
    "    criterion, performs backpropagation, and updates the model parameters using the optimizer.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        The neural network model to be trained.\n",
    "\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer used.\n",
    "\n",
    "    data_loader : DataLoader\n",
    "        A PyTorch DataLoader that provides batches of user-item interactions and ratings for training.\n",
    "\n",
    "    criterion : callable\n",
    "        A loss function.\n",
    "\n",
    "    device : torch.device\n",
    "        The device on which the model and data should be processed.\n",
    "\n",
    "    log_interval : int, optional\n",
    "        The number of steps after which to log the average loss.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    float\n",
    "        The average training loss over the epoch.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> avg_loss = train(model, optimizer, train_loader, criterion, device)\n",
    "    >>> print(f\"Average Training Loss: {avg_loss:.4f}\")\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    av_loss = []\n",
    "    train_pbar = tqdm(data_loader, smoothing=0, mininterval=1.0)\n",
    "    for i, (user, item, ratings) in enumerate(train_pbar):\n",
    "        user, item, ratings = user.to(device), item.to(device), ratings.to(device)\n",
    "        y = model(user, item)\n",
    "        loss = criterion(y, ratings.float())\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            loss = total_loss / log_interval\n",
    "            av_loss.append(loss)\n",
    "            train_pbar.set_postfix(loss=loss)\n",
    "            total_loss = 0\n",
    "    return np.mean(av_loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "106bec22-19d8-4223-86d3-3fc07f6a71a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of the given model.\n",
    "\n",
    "    This function sets the model to evaluation mode and computes the mean squared error over the provided dataloader.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        The neural network model to be evaluated.\n",
    "\n",
    "    data_loader : DataLoader\n",
    "        A PyTorch DataLoader.\n",
    "\n",
    "    device : torch.device\n",
    "        The device (CPU or GPU) on which the model and data should be processed.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    float\n",
    "        The mean squared error (MSE) between the predicted ratings and the actual ratings, scaled by a factor of 5.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> mse = test(model, test_loader, device)\n",
    "    >>> print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    targets, predicts = list(), list()\n",
    "    with torch.no_grad():\n",
    "        for user, item, ratings in tqdm(data_loader, smoothing=0, mininterval=1.0):\n",
    "            user, item, ratings = user.to(device), item.to(device), ratings.to(device)\n",
    "            y = model(user,item)\n",
    "            targets.extend(ratings.tolist())\n",
    "            predicts.extend(y.tolist())\n",
    "    return 5.* mean_squared_error(targets, predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11cef4ec-b660-4eb3-a158-d69304d73da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we instantiate the model, as well as it's optimizer and loss function.\n",
    "\"\"\"\n",
    "\n",
    "model = NCF(dataset.pair_max_ids[0]+1, dataset.pair_max_ids[1]+1, embedding_size, hidden_layers).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90cd58bc-00c5-485f-ae9c-aa66765a49bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6837"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helper to calculate the number of training steps, usefull for smaller datasets like ml-100k\n",
    "\n",
    "n_steps = math.ceil(train_length / batch_size)\n",
    "n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9ea6ec6-5955-4389-a793-fc6a239c5f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6837/6837 [00:37<00:00, 184.48it/s, loss=0.032]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6837/6837 [00:36<00:00, 188.78it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1954/1954 [00:10<00:00, 189.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 train: mae: 0.15454255308172904\n",
      "epoch: 1 validation: mae: 0.1567457369901873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6837/6837 [00:35<00:00, 194.78it/s, loss=0.0299]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6837/6837 [00:33<00:00, 205.62it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1954/1954 [00:10<00:00, 181.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 train: mae: 0.14163317444487977\n",
      "epoch: 2 validation: mae: 0.1472045720218037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6837/6837 [00:35<00:00, 194.78it/s, loss=0.028]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6837/6837 [00:34<00:00, 199.81it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1954/1954 [00:08<00:00, 225.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 train: mae: 0.13164767591907592\n",
      "epoch: 3 validation: mae: 0.13986598320870733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6837/6837 [00:33<00:00, 202.52it/s, loss=0.0266]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6837/6837 [00:34<00:00, 198.35it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1954/1954 [00:08<00:00, 242.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 train: mae: 0.12261401002467788\n",
      "epoch: 4 validation: mae: 0.13347205832157105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6837/6837 [00:34<00:00, 198.76it/s, loss=0.0258]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6837/6837 [00:34<00:00, 198.02it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1954/1954 [00:08<00:00, 226.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 train: mae: 0.117443961597783\n",
      "epoch: 5 validation: mae: 0.13040092285196567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6837/6837 [00:34<00:00, 196.67it/s, loss=0.0255]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6837/6837 [00:32<00:00, 212.93it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1954/1954 [00:09<00:00, 214.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 train: mae: 0.11447731381725743\n",
      "epoch: 6 validation: mae: 0.12885508561116038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6837/6837 [00:34<00:00, 197.23it/s, loss=0.0253]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6837/6837 [00:33<00:00, 201.80it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1954/1954 [00:11<00:00, 176.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 train: mae: 0.1132725117678936\n",
      "epoch: 7 validation: mae: 0.12839608234650082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6837/6837 [00:34<00:00, 196.70it/s, loss=0.0252]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6837/6837 [00:33<00:00, 201.71it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1954/1954 [00:08<00:00, 223.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 train: mae: 0.11283024448968164\n",
      "epoch: 8 validation: mae: 0.128229469639282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6837/6837 [00:33<00:00, 203.84it/s, loss=0.0252]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6837/6837 [00:32<00:00, 209.17it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1954/1954 [00:13<00:00, 141.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 train: mae: 0.11260593912765203\n",
      "epoch: 9 validation: mae: 0.1281538413914149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6837/6837 [00:33<00:00, 201.89it/s, loss=0.0252]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6837/6837 [00:34<00:00, 198.59it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1954/1954 [00:09<00:00, 208.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 train: mae: 0.11240976340451683\n",
      "epoch: 10 validation: mae: 0.12803944921589847\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here we train and evaluate the model.\n",
    "\"\"\"\n",
    "\n",
    "metric_values = []\n",
    "loss_values = []\n",
    "for epoch_i in range(epochs):\n",
    "    loss = train(model, optimizer, train_data_loader, criterion, device, log_interval=100)\n",
    "    loss_values.append((epoch_i, loss))\n",
    "    metric_train = test(model, train_data_loader, device)\n",
    "    metric_valid = test(model, valid_data_loader, device)\n",
    "    print('epoch:', epoch_i + 1, 'train: mae:', metric_train)\n",
    "    print('epoch:', epoch_i + 1, 'validation: mae:', metric_valid)\n",
    "    \n",
    "    \n",
    "    metric_values.append((epoch_i, metric_train, metric_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63adf82b-37e2-4027-9fee-c154138a202a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a28ca0-2094-45b9-94d3-68f9b3b3a4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
